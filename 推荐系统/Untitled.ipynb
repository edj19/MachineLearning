{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Industrial Recommendation System<font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 推荐系统介绍\n",
    "* 推荐系统评估\n",
    "1. 评估指标\n",
    "2. 评估方法\n",
    "* 推荐系统实践\n",
    "1. 如何解决冷启动问题\n",
    "2. 工业界推荐系统架构\n",
    "3. 学术界和工业界的区别\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 推荐系统：不需要用户提供明确地需求，通过分析用户的历史行为给用户的兴趣进行建模，从而主动给用户推荐能够满足他们兴趣和需求的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig2](figures/fig2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why\n",
    "\n",
    "推荐系统存在的前提：\n",
    "* 信息过载\n",
    "* 用户需求不明确\n",
    "\n",
    "推荐系统的目标\n",
    "* 高效连接用户和物品，发现长尾商品\n",
    "* 留住用户和内容生产者，实现商业目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐系统评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig1](figures/FIG1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见评估指标\n",
    "\n",
    "* 准确性\n",
    "* 满意度\n",
    "* 覆盖率\n",
    "* 多样性\n",
    "* 新颖性\n",
    "* 惊喜度\n",
    "* 信任度\n",
    "* 实时性\n",
    "* 鲁棒性\n",
    "* 可扩展性\n",
    "* 商业目标\n",
    "* 用户留存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit vs Tmplicit\n",
    "\n",
    "![fig4](figures/fig4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确性（学术界）\n",
    "* 评分预测\n",
    "$$\n",
    "RMSE=\\sqrt{\\frac{\\sum_{u,i\\in T}(|r_{ui}-\\hat{r}_{ui})^2}{|T|}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "MAE = \\frac{\\sum_{u,i\\in T}|r_{ui}-\\hat{r}_{ui}|}{|T|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* topN推荐\n",
    "$$\n",
    "Recall = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确性（工业界）\n",
    "\n",
    "![fig5](figures/fig5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "覆盖度\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多样性&新颖性&惊喜性\n",
    "\n",
    "* 多样性：推荐列表中两两物品的不相似性。（相似性如何度量）\n",
    "* 新颖性：未曾关注的类别、作者；推荐结果的平均流行度\n",
    "* 惊喜性：历史不相似（惊）但很满意（喜）\n",
    "\n",
    "往往需要牺牲准确性\n",
    "* 使用历史行为预测用户对某个物品的喜爱程度\n",
    "* 系统过度强调实时性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>Explotitation & Exploration</font>\n",
    "\n",
    "* Exploitation: 选择现有可能的最佳方案\n",
    "* Exploration: 选择现有不确定的一些方案，但未来可能会有高收益的方案\n",
    "\n",
    "在做两类决策的过程中，不断更新对现有决策的不确定性的认识，优化长期的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-armed bandit problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandit算法-原理\n",
    "\n",
    "* Epsilon-Greedy:以1-$\\epsilon$的概率选取当前受益最大的臂，以$\\epsilon$的概率随机选取一个臂\n",
    "* Upper Confidence Bound:均值越大，标准值越小，被选中的概率会越来越大\n",
    "\n",
    "$$\n",
    "x_j(t)+\\sqrt{\\frac{2\\ln t}{T_{j,t}}}\n",
    "$$\n",
    "\n",
    "* Thompson Sampling:每个臂维护一个beta(wins,lose)分布，每次用现有的beta分布产生一个随机数，选择随机数最大的臂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "number_of_bandits = 10\n",
    "number_of_arms = 10\n",
    "number_of_pulls = 10000\n",
    "epsilon = 0.3\n",
    "min_temp = 0.1\n",
    "decay_rate = 0.999\n",
    "\n",
    "def pick_arm(q_values,counts,strategy,success,failure):\n",
    "    global epsilon\n",
    "    if strategy==\"random\":\n",
    "        return np.random.randint(0,len(q_values))\n",
    "    \n",
    "    if strategy==\"greedy\":\n",
    "        best_arms_value = np.max(q_values)\n",
    "        best_arms = np.argwhere(q_values==best_arms_valuees).flatten()\n",
    "        return best_arms[np.randoma.randint(0.,len(best_arms))]\n",
    "    \n",
    "    if strategy==\"egreedy\" or strategy==\"egreedy_decay\":\n",
    "        if strategy==\"egreedy_decay\":\n",
    "            epsilon = max(epsilon*decay_rate,min_temp)\n",
    "        if np.random.random()>epsilon:\n",
    "            best_arms_value = np.max(q_values)\n",
    "            best_arms = np.argwhere(q_values==best_arms_valuee).flatten()\n",
    "            return best_arms[np.random.randint(0,len(best_arms))]\n",
    "        else:\n",
    "            return np.random.randint(0,len(q_values))\n",
    "        \n",
    "    if strategy==\"ucb\":\n",
    "        total_counts = np.sum(counts)\n",
    "        q_values_ucb = q_values + np.sqrt(np.reciprocal(counts+0.001)*2*math.log(total_counts+1.0))\n",
    "        best_arms_value = np.max(q_values_ucb)\n",
    "        best_arms = np.argwhere(q_values_ucb==best_arms_valuee).flatten()\n",
    "        return best_arms[np.random.randint(0,len(best_arms))]\n",
    "    \n",
    "    if strategy==\"thompson\":\n",
    "        sample_means = np.zeros(len(counts))\n",
    "        for i in range(len(counts)):\n",
    "            sample_means[i] = np.random.beta(success[i]+1,failure[i]+1)\n",
    "        return np.argmax(sample_means)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for st in [\"greedy\",\"random\",\"egreedy\",\"egreedy_decay\",\"ucb\",\"thompson\"]:\n",
    "    best_arm_counts = np.zeros((number_of_bandits,number_of_pulls))\n",
    "    \n",
    "    for i in range(number_of_bandits):\n",
    "        arm_means = np.random.rand(number_of_arms)\n",
    "        best_arm = np.argmax(arm_means)\n",
    "        \n",
    "        q_values = np.zeros(number_of_arms)\n",
    "        counts = np.zeros(number_of_arms)\n",
    "        success = np.zeros(number_of_arms)\n",
    "        failure = np.zeros(number_of_arms)\n",
    "        \n",
    "        for j in range(number_of_pulls):\n",
    "            a = pick_arm(q_values,counts,st,success,failure)\n",
    "            \n",
    "            reward = np.random.binomial(1,rarm_means[a])\n",
    "            counts[a] += 1.0\n",
    "            q_values[a] += (reward-q_values[a])/counts[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=blue>Bandit算法-应用</font>\n",
    "\n",
    "* 兴趣探索\n",
    "* 冷启动探索\n",
    "* LinUCB: 加入特征信息。用User和Item的特征预估回报及其置信区间，选择置信区间上界最大的Item推荐，观察回报后更新线性关系的参数，以此达到试验学习的目的。\n",
    "* COFIBA:bandit协同过滤\n",
    "\n",
    "----> * 基于用户聚类挑选最佳的Item(相似用户集成决策的Bandit);\n",
    "\n",
    "----> * 基于用户的反馈情况调整User和Item的聚类(协同过滤部分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EE实践\n",
    "\n",
    "* 兴趣扩展：相似话题，搭配推荐\n",
    "* 人群算法：userCF,用户聚类\n",
    "* Bandit算法\n",
    "* graph waiking\n",
    "* 平衡个性化推荐和热门推荐比例\n",
    "* 随机丢弃用户行为历史\n",
    "* 随机扰动模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "眼前的苟且 & 远方的田野\n",
    "\n",
    "* 探索伤害用户体验，可能导致用户流失\n",
    "* 探索带来的长期受益（留存率）评估周期长，KPI压力大\n",
    "* 如何平衡实时兴趣和长期兴趣？\n",
    "* 如何平衡短期产品体验和长期系统生态？\n",
    "* 如何平衡大众口味和小众需求？\n",
    "* 如何避免劣币趋势良币？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
